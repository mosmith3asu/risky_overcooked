{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36aba985",
   "metadata": {},
   "source": [
    "# Overcooked Tutorial\n",
    "This Notebook will demonstrate a couple of common use cases of the Overcooked Ai Library, including loading and evaluating agents and visualizing trajectories. Ideally we will have a Colab notebook you can interact with, but sadly Colab only supports python 3.10 kernel, and currently there are problems loading files pickled in 3.7 environment. As a compromise we created this notebook where you can see some examples of the most frequently used methods after they are executed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6b8ba",
   "metadata": {},
   "source": [
    "# 0): Training you agent\n",
    "The most convenient way to train an agent is with the [ppo_rllib_client.py](https://github.com/HumanCompatibleAI/overcooked_ai/blob/master/src/human_aware_rl/ppo/ppo_rllib_client.py) file, where you can either pass in the arguments through commandline, or you can directly modify the variables you want to change in the file. \n",
    "\n",
    "You can also start an experiment in another python script like the following, which can sometimes be more convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60521ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T23:23:53.235781786Z",
     "start_time": "2024-01-24T23:23:27.837921443Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mason/PycharmProjects/overcooked_ai/.venv/lib/python3.7/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/home/mason/PycharmProjects/overcooked_ai/.venv/lib/python3.7/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/home/mason/PycharmProjects/overcooked_ai/.venv/lib/python3.7/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    }
   ],
   "source": [
    "from human_aware_rl.ppo.ppo_rllib_client import ex\n",
    "# For all the tunable paramters, check out ppo_rllib_client.py file\n",
    "# Note this is not what the configuration should look like for a real experiment\n",
    "config_updates = {\n",
    "    \"results_dir\": \"path/to/results\", #change this to your local directory\n",
    "    \"layout_name\": \"cramped_room\",\n",
    "    \"clip_param\": 0.2,\n",
    "    'gamma': 0.9,\n",
    "    'num_training_iters': 10, #this should usually be a lot higher\n",
    "    'num_workers': 1,\n",
    "    'num_gpus': 0,\n",
    "    \"verbose\": False,\n",
    "    'train_batch_size': 800,\n",
    "    'sgd_minibatch_size': 800,\n",
    "    'num_sgd_iter': 1,\n",
    "    \"evaluation_interval\": 2\n",
    "}\n",
    "run = ex.run(config_updates=config_updates, options={\"--loglevel\": \"ERROR\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96de10c0",
   "metadata": {},
   "source": [
    "One can check the results of the experiment run by accessing **run.result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44455fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T23:23:53.242948669Z",
     "start_time": "2024-01-24T23:23:53.238401981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'average_sparse_reward': 0.0, 'average_total_reward': 14.290098302224868}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = run.result\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f96f8",
   "metadata": {},
   "source": [
    "In practice, the reward should be much higher if optimized. Checkout the graph in the [README](https://github.com/HumanCompatibleAI/overcooked_ai/tree/master/src/human_aware_rl) in human_aware_rl module for baseline performances.\n",
    "\n",
    "Similarly, you can train BC agents with the [reproduce_bc.py](https://github.com/HumanCompatibleAI/overcooked_ai/blob/master/src/human_aware_rl/imitation/reproduce_bc.py) file under the human_aware_rl/imitation directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f493c88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T23:24:09.598600165Z",
     "start_time": "2024-01-24T23:23:53.241904666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /home/mason/PycharmProjects/overcooked_ai/src/human_aware_rl/static/human_data/cleaned/2019_hh_trials_train.pickle\n",
      "Number of trajectories processed for each layout: {'cramped_room': 14}\n",
      "Train on 28539 samples, validate on 5037 samples\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - tensorflow - OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28539/28539 - 0s - loss: 0.9533 - sparse_categorical_accuracy: 0.7217 - val_loss: 0.8932 - val_sparse_categorical_accuracy: 0.7076 - lr: 0.0010 - 455ms/epoch - 16us/sample\n",
      "Epoch 2/10\n",
      "28539/28539 - 0s - loss: 0.8510 - sparse_categorical_accuracy: 0.7245 - val_loss: 0.8265 - val_sparse_categorical_accuracy: 0.7044 - lr: 0.0010 - 280ms/epoch - 10us/sample\n",
      "Epoch 3/10\n",
      "28539/28539 - 0s - loss: 0.8145 - sparse_categorical_accuracy: 0.7264 - val_loss: 0.8030 - val_sparse_categorical_accuracy: 0.7000 - lr: 0.0010 - 348ms/epoch - 12us/sample\n",
      "Epoch 4/10\n",
      "28539/28539 - 0s - loss: 0.7921 - sparse_categorical_accuracy: 0.7239 - val_loss: 0.7920 - val_sparse_categorical_accuracy: 0.7042 - lr: 0.0010 - 313ms/epoch - 11us/sample\n",
      "Epoch 5/10\n",
      "28539/28539 - 0s - loss: 0.7790 - sparse_categorical_accuracy: 0.7238 - val_loss: 0.7745 - val_sparse_categorical_accuracy: 0.7042 - lr: 0.0010 - 333ms/epoch - 12us/sample\n",
      "Epoch 6/10\n",
      "28539/28539 - 0s - loss: 0.7710 - sparse_categorical_accuracy: 0.7226 - val_loss: 0.7710 - val_sparse_categorical_accuracy: 0.7036 - lr: 0.0010 - 306ms/epoch - 11us/sample\n",
      "Epoch 7/10\n",
      "28539/28539 - 0s - loss: 0.7634 - sparse_categorical_accuracy: 0.7233 - val_loss: 0.7703 - val_sparse_categorical_accuracy: 0.6984 - lr: 0.0010 - 310ms/epoch - 11us/sample\n",
      "Epoch 8/10\n",
      "28539/28539 - 0s - loss: 0.7598 - sparse_categorical_accuracy: 0.7242 - val_loss: 0.7679 - val_sparse_categorical_accuracy: 0.7050 - lr: 0.0010 - 298ms/epoch - 10us/sample\n",
      "Epoch 9/10\n",
      "28539/28539 - 0s - loss: 0.7560 - sparse_categorical_accuracy: 0.7229 - val_loss: 0.7517 - val_sparse_categorical_accuracy: 0.7000 - lr: 0.0010 - 336ms/epoch - 12us/sample\n",
      "Epoch 10/10\n",
      "28539/28539 - 0s - loss: 0.7524 - sparse_categorical_accuracy: 0.7243 - val_loss: 0.7482 - val_sparse_categorical_accuracy: 0.6970 - lr: 0.0010 - 296ms/epoch - 10us/sample\n",
      "Saving bc model at  path/to/bc_dir\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.engine.functional.Functional at 0x7f4a0c080710>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = \"cramped_room\" # any compatible layouts \n",
    "from human_aware_rl.imitation.behavior_cloning_tf2 import (\n",
    "    get_bc_params, # get the configuration for BC agents\n",
    "    train_bc_model, # train the BC model\n",
    ")\n",
    "from human_aware_rl.static import (\n",
    "    CLEAN_2019_HUMAN_DATA_TRAIN, # human trajectories\n",
    ")\n",
    "\n",
    "params_to_override = {\n",
    "    # this is the layouts where the training will happen\n",
    "    \"layouts\": [layout], \n",
    "    # this is the layout that the agents will be evaluated on\n",
    "    # Most of the time they should be the same, but because of refactoring some old layouts have more than one name and they need to be adjusted accordingly\n",
    "    \"layout_name\": layout, \n",
    "    \"data_path\": CLEAN_2019_HUMAN_DATA_TRAIN,\n",
    "    \"epochs\": 10,\n",
    "    \"old_dynamics\": True,\n",
    "}\n",
    "\n",
    "bc_params = get_bc_params(**params_to_override)\n",
    "train_bc_model(\"path/to/bc_dir\", bc_params, verbose = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc068ebc",
   "metadata": {},
   "source": [
    "# 1): Loading trained agents\n",
    "This section will show you how to load a pretrained agents. To load an agent, you can use the load_agent function in the [rllib.py](https://github.com/HumanCompatibleAI/overcooked_ai/blob/master/src/human_aware_rl/rllib/rllib.py) file. For the purpose of demonstration, I will be loading a local agent, which is also one of the agents included in the web demo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844332c3",
   "metadata": {},
   "source": [
    "## 1.1): Loading PPO agent\n",
    "The PPO agents are all trained via the Ray trainer, so to load a trained agent, we can just use the load_agent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aeaae41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T23:24:11.006798506Z",
     "start_time": "2024-01-24T23:24:09.600756859Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 16:24:10,293\tWARNING deprecation.py:48 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/plain": "<human_aware_rl.rllib.rllib.RlLibAgent at 0x7f4ac0011ed0>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.rllib.rllib import load_agent\n",
    "agent_path = \"src/overcooked_demo/server/static/assets/agents/RllibCrampedRoomSP/agent\"\n",
    "# The first argument is the path to the saved trainer, we then loads the agent associated with that trainner\n",
    "## If you use the experiment setup provided, the saved path should be the results_dir in the configuration\n",
    "# The second argument is the type of agent to load, which only matters if it is not a self-play agent \n",
    "# The third argument is the agent_index, which is not directly related to the training\n",
    "## It is used in creating the RllibAgent class that is used for evaluation\n",
    "ppo_agent = load_agent(agent_path,\"ppo\",0)\n",
    "ppo_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143edeb6",
   "metadata": {},
   "source": [
    "This function loads an agent from the trainer. The RllibAgent class is a wrapper around the core policy, which simplifies pairing and evaluating different type of agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a9df6",
   "metadata": {},
   "source": [
    "## 1.2) Loading BC agent\n",
    "The BC (behavior cloning) agents are trained separately without using Ray. We showed how to train a BC agent in the previous section, and to load a trained agent, we can use the load_bc_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94ab2a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T23:24:11.025473502Z",
     "start_time": "2024-01-24T23:24:11.008187487Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at src/human_aware_rl/imitation/bc_runs/train/cramped_room",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_118759/731591246.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m#this is the same path you used when training the BC agent\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mbc_model_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"src/human_aware_rl/imitation/bc_runs/train/cramped_room\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mbc_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbc_params\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_bc_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbc_model_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mbc_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbc_params\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/overcooked_ai/src/human_aware_rl/imitation/behavior_cloning_tf2.py\u001B[0m in \u001B[0;36mload_bc_model\u001B[0;34m(model_dir, verbose)\u001B[0m\n\u001B[1;32m    298\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    299\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Loading bc model from \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 300\u001B[0;31m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcustom_objects\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m\"tf\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    301\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"metadata.pickle\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"rb\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    302\u001B[0m         \u001B[0mbc_params\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/overcooked_ai/.venv/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     68\u001B[0m             \u001B[0;31m# To get the full stack trace, call:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m             \u001B[0;31m# `tf.debugging.disable_traceback_filtering()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 70\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     71\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m             \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/overcooked_ai/.venv/lib/python3.7/site-packages/keras/saving/save.py\u001B[0m in \u001B[0;36mload_model\u001B[0;34m(filepath, custom_objects, compile, options)\u001B[0m\n\u001B[1;32m    225\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_str\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    226\u001B[0m                         raise IOError(\n\u001B[0;32m--> 227\u001B[0;31m                             \u001B[0;34mf\"No file or directory found at {filepath_str}\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    228\u001B[0m                         )\n\u001B[1;32m    229\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: No file or directory found at src/human_aware_rl/imitation/bc_runs/train/cramped_room"
     ]
    }
   ],
   "source": [
    "from human_aware_rl.imitation.behavior_cloning_tf2 import load_bc_model\n",
    "#this is the same path you used when training the BC agent\n",
    "bc_model_path = \"src/human_aware_rl/imitation/bc_runs/train/cramped_room\"\n",
    "bc_model, bc_params = load_bc_model(bc_model_path)\n",
    "bc_model, bc_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20526ac6",
   "metadata": {},
   "source": [
    "Now that we have loaded the model, since we used Tensorflow to train the agent, we need to wrap it so it is compatible with other agents. We can do it by converting it to a Rllib-compatible policy class, and wraps it as a RllibAgent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c37a25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T23:24:11.064844092Z",
     "start_time": "2024-01-24T23:24:11.026600120Z"
    }
   },
   "outputs": [],
   "source": [
    "from human_aware_rl.imitation.behavior_cloning_tf2 import _get_base_ae, BehaviorCloningPolicy\n",
    "bc_policy = BehaviorCloningPolicy.from_model(\n",
    "        bc_model, bc_params, stochastic=True\n",
    "    )\n",
    "# We need the featurization function that is specifically defined for BC agent\n",
    "# The easiest way to do it is to create a base environment from the configuration and extract the featurization function\n",
    "# The environment is also needed to do evaluation\n",
    "\n",
    "base_ae = _get_base_ae(bc_params)\n",
    "base_env = base_ae.env\n",
    "\n",
    "from human_aware_rl.rllib.rllib import RlLibAgent\n",
    "bc_agent = RlLibAgent(bc_policy,0,base_env.featurize_state_mdp)\n",
    "bc_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c5687",
   "metadata": {},
   "source": [
    "Now we have a BC agent that is ready for evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73698e65",
   "metadata": {},
   "source": [
    "## 1.3) Loading & Creating Agent Pair\n",
    "\n",
    "To do evaluation, we need a pair of agents, or an AgentPair. We can directly load a pair of agents for evaluation, which we can do with the load_agent_pair function, or we can create an AgentPair manually from 2 separate RllibAgent instance. To directly load an AgentPair from a trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3f7d6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-24T23:24:11.028056668Z"
    }
   },
   "outputs": [],
   "source": [
    "from human_aware_rl.rllib.rllib import load_agent_pair\n",
    "# if we want to load a self-play agent\n",
    "ap_sp = load_agent_pair(agent_path,\"ppo\",\"ppo\")\n",
    "ap_sp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249778ce",
   "metadata": {},
   "source": [
    "This is convenient when the agents trained are not self-play agents. For example, if we have a PPO agent trained with a BC agent, we can load both as an agent pair at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78bb724",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-24T23:24:11.029448835Z"
    }
   },
   "outputs": [],
   "source": [
    "bc_agent_path = \"src/overcooked_demo/server/static/assets/agents/RllibCrampedRoomBC/agent\"\n",
    "ap_bc = load_agent_pair(bc_agent_path,\"ppo\",\"bc\")\n",
    "ap_bc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd83bc",
   "metadata": {},
   "source": [
    "To create an AgentPair manually, we can just pair together any 2 RllibAgent object. For example, we have created a **ppo_agent** and a **bc_agent**. To pair them up, we can just construct an AgentPair with them as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acdeee",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-24T23:24:11.075809214Z"
    }
   },
   "outputs": [],
   "source": [
    "from human_aware_rl.rllib.rllib import AgentPair\n",
    "ap = AgentPair(ppo_agent,bc_agent)\n",
    "ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6cafa",
   "metadata": {},
   "source": [
    "# 2): Evaluating AgentPair\n",
    "\n",
    "To evaluate an AgentPair, we need to first create an AgentEvaluator. You can create an AgentEvaluator in various ways, but the simpliest way to do so is from the layout_name. \n",
    "\n",
    "You can modify the settings of the layout by changing the **mdp_params** argument, but most of the time you should only need to include \"layout_name\", which is the layout you want to evaluate the agent pair on, and \"old_dynamics\", which determines whether the envrionment conforms to the design in the Neurips2019 paper, or whether the cooking should start automatically when all ingredients are present.  \n",
    "\n",
    "For the **env_params**, you can change how many steps are there in one evaluation. The default is 400, which means the game runs for 400 timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95787dc6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-24T23:24:11.076142862Z"
    }
   },
   "outputs": [],
   "source": [
    "from overcooked_ai_py.agents.benchmarking import AgentEvaluator\n",
    "# Here we create an evaluator for the cramped_room layout\n",
    "layout = \"cramped_room\"\n",
    "ae = AgentEvaluator.from_layout_name(mdp_params={\"layout_name\": layout, \"old_dynamics\": True}, \n",
    "                                     env_params={\"horizon\": 400})\n",
    "ae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471aeda",
   "metadata": {},
   "source": [
    "To run evaluations, we can use the evaluate_agent_pair method associated with the AgentEvaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93676beb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-24T23:24:11.076469015Z"
    }
   },
   "outputs": [],
   "source": [
    "# ap_sp: The AgentPair we created earlier\n",
    "# 10: how many times we should run the evaluation since the policy is stochastic\n",
    "# 400: environment timestep horizon, \n",
    "## should not be necessary is the AgentEvaluator is created with a horizon, but good to have for clarity\n",
    "result = ae.evaluate_agent_pair(ap_sp,10,400)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b6cca",
   "metadata": {},
   "source": [
    "The result returned by the AgentEvaluator contains detailed information about the evaluation runs, including actions taken by each agent at each timestep. Usually you don't need to directly interact with them, but the most direct performance measures can be retrieved with result[\"ep_returns\"], which returns the average sparse reward of each evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed7df5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-24T23:24:11.076685832Z"
    }
   },
   "outputs": [],
   "source": [
    "result[\"ep_returns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f7842",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-24T23:24:11.077004742Z"
    }
   },
   "outputs": [],
   "source": [
    "# we can use any AgentPair class, like the ap_bc object we created earlier\n",
    "# as we can see the performance is not as good as the self-play agents\n",
    "result = ae.evaluate_agent_pair(ap_bc,10,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898bae8",
   "metadata": {},
   "source": [
    "# 3): Visualization\n",
    "\n",
    "We can also visualize the trajectories of agents. One way is to run the web demo with the agents you choose, and the specific instructions can be found in the [overcooked_demo](https://github.com/HumanCompatibleAI/overcooked_ai/tree/master/src/overcooked_demo) module, which requires some setup. Another simpler way is to use the StateVisualizer, which uses the information returned by the AgentEvaluator to create a simple dynamic visualization. You can checkout [this Colab Notebook](https://colab.research.google.com/drive/1AAVP2P-QQhbx6WTOnIG54NXLXFbO7y6n#scrollTo=6Xlu54MkiXCR) that let you play with fixed agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d0c84",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-24T23:24:11.077223253Z"
    }
   },
   "outputs": [],
   "source": [
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "# here we use the self-play agentPair created earlier again\n",
    "trajs = ae.evaluate_agent_pair(ap_sp,10,400)\n",
    "StateVisualizer().display_rendered_trajectory(trajs, ipython_display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b62122",
   "metadata": {},
   "source": [
    "This should spawn a window where you can see what the agents are doing at each timestep. You can drag the slider to go forward and backward in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d470037",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T23:24:11.077666346Z",
     "start_time": "2024-01-24T23:24:11.077472922Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c87371",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-24T23:24:11.077848839Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22e439",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T23:24:11.078071407Z",
     "start_time": "2024-01-24T23:24:11.078058393Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea0d33f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T23:24:11.079415383Z",
     "start_time": "2024-01-24T23:24:11.078251836Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106aba2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-24T23:24:11.078447714Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0839354",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-24T23:24:11.078625048Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891c797",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-24T23:24:11.078797121Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
