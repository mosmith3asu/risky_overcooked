---

#TODO: Make this stucture mirror Object definitions

'ALGORITHM': 'DDQN-CPT'

# Env Params ----------------
env:
      'LAYOUT': "risky_coordination_ring"
      'HORIZON': 200
      "p_slip": 0.1
      'time_cost': -0.2                      # cost per time step (<0: -0.2 ==> 1soup/100steps)
      'obs_shape': nan # Overridden during execution depending on layout

# Learning Params ----------------
train:
    'ITERATIONS': 15_000 # total number of training iterations
    'warmup_transitions': 20_000            # number of transitions in mem before training starts (div. by horizon = iterations) #TODO: change to iterations
    "rand_start_sched": [0.0, 0.0, 1]       # percentage of ITERATIONS with random start states TODO: remove
    'epsilon_sched': [1.0, 0.15, 3_000]     # epsilon-greedy range (start,end, dur)
    'rshape_sched': [1, 0, 5_000]           # reward shaping decay (start,end, dur)
    'rationality_sched': [10, 10, 1]        # rationality level range (start,end, dur) (end=test) TODO: remove

    #TODO: move to model?
    'lr_sched': [0.0001, 0.0001, 1]   # learning rat schedule (start, end, dur) TODO: remove
    'gamma': 0.95                     # discount factor
    'tau': 0.005                      # soft update weight of target network
    "replay_memory_size": 100_000     # size of replay memory
    "minibatch_size": 256             # size of mini-batches

    curriculum: # TODO: add ALL
      schedule_decay: 0.7     # gain on starting value in schedules after first curriculum
      curriculum_mem: 10      # number of iterations to check curriculum over
      sampling_decay: 0.5     # recursive decay on likelihood of sampling past curriculum
      p_random_counter_obj: 0 # probability of a random object being on any counter
      subtask_goals:          # average number of soups delivered over curriculum_mem needed to advance
        'deliver_soup': 4.5
        'pick_up_soup': 4
        'pick_up_dish': 3.5
        'wait_to_cook': 3 #2.5
        'deliver_onion3': 2.5
        'pick_up_onion3': 2.5
        'deliver_onion2': 2
        'pick_up_onion2': 2
        'deliver_onion1': 2
        'full_task': 999 # pick_up_onion1 = full_task

agents:
      'feasible_actions': True # constrain random sampling to only feasible actions TODO: add
      'rationality': 10 # rationality level/decision temperature TODO: remove

      model:
        "num_hidden_layers": 5            # MLP params
        "size_hidden_layers": 128 #256         # MLP params
        "activation": "ReLU"              # activation function        TODO: add to config
        'clip_grad': 0.75 # with norm clip


      'equilibrium': #using level-k quant response equilibrium
        'level-k': 8 # number of recursions
        'belief_trick': True # other agent uses  level-(k+1) to avoid recomputing

      'cpt':   # Default rational CPT parameters
          'b': 0.0      # reference point
          'lam': 1.0    # loss aversion
          'eta_p': 1.0  # diminishing gain sensitivity
          'eta_n': 1.0  # diminishing loss sensitivity
          'delta_p': 1.0 # s-shaped probability estimation bias for gains
          'delta_n': 1.0 # s-shaped probability estimation bias for losses



interface:
    'enable_report': True # enables command line reporting
    'note': ''  # additional notes presented in RL-Logger

save:
    'loads': ''         # loaded model
    'fname_ext': '' # additional filename extension
    'fname_fmt': '{algorithm}{val}_{env}{val}_{pslip}{val}_{rshape}{val}_{epsilon}{val}_{lr}_{note}_{fname_ext'#TODO: add?
    'save_dir': '\\risky_overcooked_rl\\models\\'
    'wait_for_close': True # halt execution after done training until user closes the window
    'auto_save': False      # automatically save the model after training + logger closed
    'checkpoint_mem': 10    # number of iterations to average checkpoint check over (keeps latest)