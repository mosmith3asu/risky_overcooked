---
  'ALGORITHM': 'DDQN-CPT'

  # Env Params ----------------
  'LAYOUT': "risky_coordination_ring"
  'HORIZON': 200
  'ITERATIONS': 30_000
  "p_slip": 0.1
  'loads': ''         # loaded model

  # Learning Params ----------------
  "rand_start_sched": [0.0, 0.0, 10_000]  # percentage of ITERATIONS with random start states
  'epsilon_sched': [1.0, 0.15, 2000]      # epsilon-greedy range (start,end, dur)
  'rshape_sched': [1, 0, 5_000]           # reward shaping decay (start,end, dur)
  'rationality_sched': [10, 10, 10_000]   # rationality level range (start,end, dur) (end=test)
  'lr_sched': [0.0001, 0.0001, 1_000]         # learning rat schedule (start, end, dur)
#  'lr_sched': [0.00000000000001, 0.00000000000001, 1_000]         # learning rat schedule (start, end, dur)


  'gamma': 0.95                     # discount factor
  'tau': 0.005                      # soft update weight of target network
  "num_hidden_layers": 5            # MLP params
  "size_hidden_layers": 256         # MLP params
  "minibatch_size": 256             # size of mini-batches
  "replay_memory_size": 20_000      # size of replay memory
  'clip_grad': 100
  'cpt_params':   # Default rational CPT parameters
      'b': 0.0      # reference point
      'lam': 1.0    # loss aversion
      'eta_p': 1.0  # diminishing gain sensitivity
      'eta_n': 1.0  # diminishing loss sensitivity
      'delta_p': 1.0 # s-shaped probability estimation bias for gains
      'delta_n': 1.0 # s-shaped probability estimation bias for losses
  'cpt_rational_ref': False # whether to use rational reference for cpt[b]
  'note': ''  # additional notes'